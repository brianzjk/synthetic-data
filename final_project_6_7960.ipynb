{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "f54n6fOFXbHc",
        "O1CUuU5NqV3w",
        "L-VZEkdXP8hs",
        "UUNsIlf2KOBi",
        "j1ia5RaZPHJp",
        "PyMNIpVgPz_h",
        "a441hWK4QvPE",
        "wGDCtVxfQyxI",
        "TG-RmF_aQida"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMT9Zz+g2HoqohhabVObNWU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Sy9dOSeXGx8"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "from datasets import load_dataset\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from scipy.stats import kendalltau\n",
        "import spacy\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import time\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "Loading in model (we use GPT-2)"
      ],
      "metadata": {
        "id": "f54n6fOFXbHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "config = AutoConfig.from_pretrained(model_name, output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, config=config)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Set pad_token to eos_token for GPT-2 tokenizer to avoid padding errors\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "KJL7vcC3XQUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_token_spans(text, tokenizer):\n",
        "    \"\"\"\n",
        "    Splits text into sentences and maps them to token ranges.\n",
        "    Returns: List of (sentence_text, start_token_idx, end_token_idx)\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "    # Tokenize the full text with offsets to align\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True)\n",
        "    offset_mapping = inputs.offset_mapping[0].numpy() # (num_tokens, 2)\n",
        "\n",
        "    sent_spans = []\n",
        "    current_sent_idx = 0\n",
        "\n",
        "    # Simple alignment: Find which tokens fall into the char range of the sentence\n",
        "    # Note: This is a greedy alignment; production code needs robust char-to-token mapping\n",
        "    char_cursor = 0\n",
        "    for sent in sentences:\n",
        "        start_char = text.find(sent, char_cursor)\n",
        "        end_char = start_char + len(sent)\n",
        "        char_cursor = end_char\n",
        "\n",
        "        # Find tokens that overlap with this sentence range\n",
        "        sent_token_indices = []\n",
        "        for i, (start, end) in enumerate(offset_mapping):\n",
        "            # Check overlap\n",
        "            if start >= start_char and end <= end_char:\n",
        "                sent_token_indices.append(i)\n",
        "\n",
        "        if sent_token_indices:\n",
        "            sent_spans.append({\n",
        "                \"text\": sent,\n",
        "                \"start\": sent_token_indices[0],\n",
        "                \"end\": sent_token_indices[-1]\n",
        "            })\n",
        "\n",
        "    return inputs, sent_spans"
      ],
      "metadata": {
        "id": "5Z-uehJoXfBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tester for the above code"
      ],
      "metadata": {
        "id": "ygh6FgA7mEmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"The CPU handles the instructions. \"\n",
        "    \"The RAM stores the data temporarily. \"\n",
        "    \"However, the GPU is needed for rendering images. \"\n",
        "    \"Together, these components power the computer. \"\n",
        "    \"The user interacts via the keyboard.\"\n",
        ")\n",
        "\n",
        "inputs, sent_data = get_sentence_token_spans(text, tokenizer)\n",
        "print(f\"Detected {len(sent_data)} sentences.\")"
      ],
      "metadata": {
        "id": "Auc8_PZHl1T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code runs the model once, extracts the attention, and then changes the attention from a (tokens x tokens) matrix to the sentence matrix (sentences x sentences)."
      ],
      "metadata": {
        "id": "5CvGT_0FmL13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sentence_dependency_matrix(model, inputs, sent_data, agg_method='mean'):\n",
        "    \"\"\"\n",
        "    Runs the model and aggregates token-level attention into sentence-level attention.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "    # outputs.attentions is a tuple of (num_layers) tensors of shape (batch, heads, seq, seq)\n",
        "    # We will average across all layers and heads for a 'global' attention view.\n",
        "    # You could also target specific 'induction heads' if you wanted to be fancy.\n",
        "\n",
        "    # Stack layers -> (layers, batch, heads, seq, seq)\n",
        "    all_attentions = torch.stack(outputs.attentions)\n",
        "\n",
        "    # Average over layers and heads -> (seq, seq)\n",
        "    # Squeeze batch dimension (0)\n",
        "    avg_attention = all_attentions.mean(dim=(0, 2)).squeeze(0).cpu().numpy()\n",
        "\n",
        "    n_sentences = len(sent_data)\n",
        "    sent_attn_matrix = np.zeros((n_sentences, n_sentences))\n",
        "\n",
        "    for i in range(n_sentences): # Query Sentence (The one 'looking back')\n",
        "        for j in range(n_sentences): # Key Sentence (The one being looked at)\n",
        "\n",
        "            # Get token ranges\n",
        "            q_start, q_end = sent_data[i]['start'], sent_data[i]['end']\n",
        "            k_start, k_end = sent_data[j]['start'], sent_data[j]['end']\n",
        "\n",
        "            # Extract sub-matrix for these two sentences\n",
        "            # Note: causal masking means attn is 0 if k > q, handled naturally by the model\n",
        "            sub_matrix = avg_attention[q_start:q_end+1, k_start:k_end+1]\n",
        "\n",
        "            if sub_matrix.size > 0:\n",
        "                if agg_method == 'max':\n",
        "                    score = np.max(sub_matrix)\n",
        "                elif agg_method == 'mean':\n",
        "                    score = np.mean(sub_matrix)\n",
        "                elif agg_method == 'sum':\n",
        "                    score = np.sum(sub_matrix)\n",
        "\n",
        "                sent_attn_matrix[i, j] = score\n",
        "\n",
        "    return sent_attn_matrix"
      ],
      "metadata": {
        "id": "XItnuJCHma6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_matrix = extract_sentence_dependency_matrix(model, inputs, sent_data, agg_method='mean')"
      ],
      "metadata": {
        "id": "BTvNGZ_3mdpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of Sentence Attention\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(attn_matrix, cmap='viridis')\n",
        "plt.title(\"Sentence-to-Sentence Attention (Raw)\")\n",
        "plt.xlabel(\"Key Sentence (Source Info)\")\n",
        "plt.ylabel(\"Query Sentence (Destination Info)\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DjxACTr7mfEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we construct the DAG of the attentions. In this DAG, there's an edge between two sentences if the attention is above some certain threshold. This is because any two sentences will likely have some small attention, so we only want to filter for sentence pairs that are highly important to each other."
      ],
      "metadata": {
        "id": "G-GMXHoqnKyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_permutations(sent_data, attn_matrix, quantile_threshold=0.2, n_samples=5):\n",
        "    \"\"\"\n",
        "    Builds the dependency DAG and samples 'n_samples' unique permutations.\n",
        "    \"\"\"\n",
        "    G = nx.DiGraph()\n",
        "    n = len(sent_data)\n",
        "\n",
        "    # 1. Add Nodes\n",
        "    for i in range(n):\n",
        "        G.add_node(i, text=sent_data[i]['text'])\n",
        "\n",
        "    # 2. Determine Threshold\n",
        "    lower_triangle = attn_matrix[np.tril_indices(n, k=-1)]\n",
        "    if len(lower_triangle) > 0:\n",
        "        cutoff = np.quantile(lower_triangle, quantile_threshold)\n",
        "    else:\n",
        "        cutoff = 0.0\n",
        "\n",
        "    # 3. Add Edges (Pruning weak attention)\n",
        "    # Edge j -> i means i depends on j\n",
        "    for i in range(n): # Later sentence\n",
        "        for j in range(i): # Earlier sentence\n",
        "            if attn_matrix[i, j] > cutoff:\n",
        "                G.add_edge(j, i)\n",
        "\n",
        "    # 4. Sample Permutations\n",
        "    unique_permutations = set()\n",
        "    attempts = 0\n",
        "    max_attempts = n_samples * 5 # Stop if we can't find enough unique ones\n",
        "\n",
        "    # Add the original sequence as the first baseline \"permutation\"\n",
        "    original_seq = tuple(range(n))\n",
        "    unique_permutations.add(original_seq)\n",
        "\n",
        "    while len(unique_permutations) < (n_samples + 1) and attempts < max_attempts:\n",
        "        # Call the internal sampler\n",
        "        new_perm = sample_topological_sort(G)\n",
        "        unique_permutations.add(tuple(new_perm))\n",
        "        attempts += 1\n",
        "\n",
        "    # Convert sets of tuples back to lists\n",
        "    results = [list(p) for p in unique_permutations]\n",
        "\n",
        "    return G, results"
      ],
      "metadata": {
        "id": "CuIVqbAemiiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there's $O(n!)$ possible permutations, we limit the permutations we use to some random sample of all valid topological ordering of the vertices of the DAG."
      ],
      "metadata": {
        "id": "KOF1isB3nfVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_topological_sort(G):\n",
        "    \"\"\"\n",
        "    Generates a single random topological sort of a DAG using\n",
        "    Randomized Kahn's Algorithm.\n",
        "\n",
        "    Args:\n",
        "        G: A NetworkX DiGraph.\n",
        "    Returns:\n",
        "        A list of node indices representing a valid permutation.\n",
        "    \"\"\"\n",
        "    in_degree = {u: d for u, d in G.in_degree() if d > 0}\n",
        "\n",
        "    # Initialize queue with nodes having 0 in-degree (no dependencies)\n",
        "    zero_in_degree_queue = [n for n in G.nodes() if G.in_degree(n) == 0]\n",
        "\n",
        "    sorted_order = []\n",
        "\n",
        "    while zero_in_degree_queue:\n",
        "        # Pick a random candidate from the available \"next steps\"\n",
        "        candidate_idx = random.randrange(len(zero_in_degree_queue))\n",
        "        current_node = zero_in_degree_queue.pop(candidate_idx)\n",
        "\n",
        "        sorted_order.append(current_node)\n",
        "\n",
        "        # 'Virtual' removal of the node: update neighbor in-degrees\n",
        "        for neighbor in G.successors(current_node):\n",
        "            in_degree[neighbor] -= 1\n",
        "            if in_degree[neighbor] == 0:\n",
        "                zero_in_degree_queue.append(neighbor)\n",
        "                del in_degree[neighbor]\n",
        "\n",
        "    # Cycle detection fallback\n",
        "    if len(sorted_order) != len(G.nodes()):\n",
        "        # This usually means the graph had a cycle.\n",
        "        # Fallback: return the nodes sorted by their original index (0, 1, 2...)\n",
        "        return sorted(list(G.nodes()))\n",
        "\n",
        "    return sorted_order"
      ],
      "metadata": {
        "id": "CMQ-1i02pfgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dag, permutations = generate_permutations(\n",
        "    sent_data,\n",
        "    attn_matrix,\n",
        "    quantile_threshold=0.5,\n",
        "    n_samples=3\n",
        ")\n",
        "\n",
        "print(f\"Generated {len(permutations)} unique sequences (including original).\\n\")"
      ],
      "metadata": {
        "id": "riSlbeEUnR0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing out 3 examples of the generated sentences:"
      ],
      "metadata": {
        "id": "HaAi-TvmqCij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, perm in enumerate(permutations[:3]):\n",
        "    print(f\"--- Permutation {i+1} ---\")\n",
        "    reconstructed_text = \" \".join([sent_data[idx]['text'] for idx in perm])\n",
        "    print(reconstructed_text)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "lQZmWL1bnYV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a visualization of what the DAG looks like for this example."
      ],
      "metadata": {
        "id": "LMVokRXwoF5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "pos = nx.spring_layout(dag)\n",
        "nx.draw(dag, pos, with_labels=True, node_color='lightblue', edge_color='gray', arrowsize=20)\n",
        "plt.title(\"Semantic Dependency DAG\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vdW6hk6cnbcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real Data Example\n",
        "Now that we have a working setup, we can try this on some synthetic data!"
      ],
      "metadata": {
        "id": "O1CUuU5NqV3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loader(dataset_name=\"wikitext\", subset=\"wikitext-103-raw-v1\", split=\"train\"):\n",
        "    \"\"\"\n",
        "    Returns an iterable streaming dataset.\n",
        "    \"\"\"\n",
        "    # streaming=True means we don't download the 100GB+ file to disk\n",
        "    dataset = load_dataset(dataset_name, subset, split=split, streaming=True)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "_u7jHC_vqgPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize loader\n",
        "raw_dataset = get_data_loader()\n",
        "\n",
        "# Create a generator to fetch documents one by one\n",
        "data_iter = iter(raw_dataset)"
      ],
      "metadata": {
        "id": "L0N5-pPUqlac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: you need to run the below cell multiple times until you get something that actually looks like part of a Wikipedia article."
      ],
      "metadata": {
        "id": "t7EOgi2WsQRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fetching document from Wikitext-103...\")\n",
        "sample = next(data_iter)\n",
        "doc_text = sample['text']\n",
        "print(f\"\\nOriginal Doc Start: {doc_text[:100]}...\\n\")"
      ],
      "metadata": {
        "id": "vi1vDagxqk4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating the permutations!"
      ],
      "metadata": {
        "id": "gkw_0xt7sVXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, sent_data = get_sentence_token_spans(doc_text, tokenizer)\n",
        "print(f\"Detected {len(sent_data)} sentences.\")\n",
        "\n",
        "attn_matrix = extract_sentence_dependency_matrix(model, inputs, sent_data)\n",
        "dag, permutations = generate_permutations(sent_data, attn_matrix, quantile_threshold=0.67)\n",
        "\n",
        "print(f\"Generated {len(permutations)} permutations (including original).\")"
      ],
      "metadata": {
        "id": "-pLunqi_ndOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, perm in enumerate(permutations[:3]):\n",
        "    print(f\"--- Permutation {i+1} ---\")\n",
        "    reconstructed_text = \" \".join([sent_data[idx]['text'] for idx in perm])\n",
        "    print(reconstructed_text)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "Qs0vGtEisX-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "pos = nx.spring_layout(dag)\n",
        "nx.draw(dag, pos, with_labels=True, node_color='lightblue', edge_color='gray', arrowsize=20)\n",
        "plt.title(\"Semantic Dependency DAG\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iBnp_9zTq-2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "D5CWNrw7tsri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup for training"
      ],
      "metadata": {
        "id": "L-VZEkdXP8hs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training hyperparameters:"
      ],
      "metadata": {
        "id": "dWGrjedKI2AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LEARNING_RATE = 5e-5\n",
        "NUM_STEPS = 1000\n",
        "PERMUTATIONS_PER_DOC = 3 # K samples\n",
        "ATTENTION_THRESHOLD = 0.67"
      ],
      "metadata": {
        "id": "nvrlZ9purApf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting ready to train"
      ],
      "metadata": {
        "id": "7OnKe7CQI35l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(DEVICE)\n",
        "model.train()\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "running_loss = 0.0\n",
        "total_tokens_processed = 0\n",
        "start_time = time.time()\n",
        "\n",
        "print(f\"Starting training on {DEVICE}...\")\n",
        "print(f\"Strategy: 1 Original + {PERMUTATIONS_PER_DOC} Synthetic per step.\")\n",
        "\n",
        "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", streaming=True)\n",
        "data_iter = iter(dataset)"
      ],
      "metadata": {
        "id": "aU5biWC6Ibb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original Training loop"
      ],
      "metadata": {
        "id": "UUNsIlf2KOBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(NUM_STEPS):\n",
        "    try:\n",
        "        # 1. Fetch Data\n",
        "        sample = next(data_iter)\n",
        "        text = sample['text']\n",
        "\n",
        "        # Skip garbage/short data\n",
        "        if len(text) < 200:\n",
        "            continue\n",
        "\n",
        "        # 2. Preprocessing & Alignment\n",
        "        # We need the inputs on the device for the 'diagnostic' pass\n",
        "        inputs, sent_data = get_sentence_token_spans(text, tokenizer)\n",
        "        if len(sent_data) < 5: # Need enough sentences to permute\n",
        "            continue\n",
        "\n",
        "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "\n",
        "        # 3. Diagnostic Pass (No Grad)\n",
        "        # We put model in eval temporarily to get clean attention maps\n",
        "        model.eval()\n",
        "        attn_matrix = extract_sentence_dependency_matrix(model, inputs, sent_data)\n",
        "        model.train() # Switch back to train\n",
        "\n",
        "        # 4. Generate Synthetic Data\n",
        "        # Returns [Original_Indices, Perm1, Perm2...]\n",
        "        _, permutations = generate_permutations(\n",
        "            sent_data,\n",
        "            attn_matrix,\n",
        "            quantile_threshold=ATTENTION_THRESHOLD,\n",
        "            n_samples=PERMUTATIONS_PER_DOC\n",
        "        )\n",
        "\n",
        "        # 5. Prepare Training Batch\n",
        "        # We re-tokenize the permutations.\n",
        "        # Why? Because BPE merges might change when sentence neighbors change.\n",
        "        batch_texts = []\n",
        "        for perm_indices in permutations:\n",
        "            # Reconstruct string\n",
        "            perm_text = \" \".join([sent_data[idx]['text'] for idx in perm_indices])\n",
        "            batch_texts.append(perm_text)\n",
        "\n",
        "        # Tokenize the batch of (1 Real + K Synthetic)\n",
        "        batch_inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=1024\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # 6. Training Step\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass on the mixed batch\n",
        "        outputs = model(**batch_inputs, labels=batch_inputs[\"input_ids\"])\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping to prevent exploding gradients from bad permutations\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # 7. Logging\n",
        "        current_loss = loss.item()\n",
        "        running_loss += current_loss\n",
        "        batch_tokens = batch_inputs[\"input_ids\"].numel()\n",
        "        total_tokens_processed += batch_tokens\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            if len(permutations) > 1:\n",
        "              original_indices = permutations[0]\n",
        "              synthetic_indices = permutations[1]\n",
        "\n",
        "              # Calculate Kendall Tau\n",
        "              # 1.0 = Same order, -1.0 = Reverse order, 0.0 = Random\n",
        "              tau, _ = kendalltau(original_indices, synthetic_indices)\n",
        "\n",
        "              # Convert to a \"Scramble Score\" (0 = Identical, 1 = Very scrambled)\n",
        "              # Using (1 - tau) / 2 to normalize\n",
        "              scramble_score = (1 - tau) / 2\n",
        "\n",
        "              print(f\"   > Debug: Scramble Score: {scramble_score:.2f} (Tau: {tau:.2f})\")\n",
        "\n",
        "            avg_loss = running_loss / 10 if step > 0 else current_loss\n",
        "            elapsed = time.time() - start_time\n",
        "            tokens_per_sec = total_tokens_processed / elapsed\n",
        "\n",
        "            print(f\"Step {step} | Loss: {avg_loss:.4f} | Tok/s: {tokens_per_sec:.0f} | \"\n",
        "                  f\"Permutations: {len(batch_texts)}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    except StopIteration:\n",
        "        print(\"Dataset exhausted.\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping step due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "gV_TXX6rIh6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ltvfSdUUI6Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic Threshold"
      ],
      "metadata": {
        "id": "j1ia5RaZPHJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In the beginning, the model's attention maps are messy and not very semantic.\n",
        "\n",
        "* **Concept:** Start with a `quantile_threshold` of **0.9** (very strict, graph is almost linear, very few permutations).\n",
        "\n",
        "* **Evolution:** Linearly decrease the threshold to **0.2** over the course of training.\n",
        "\n",
        "* **Why:** As the model learns better causal dependencies, its attention matrix becomes cleaner. You can then trust it to allow more aggressive permutations later in training."
      ],
      "metadata": {
        "id": "00XQmPouPpt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dynamic_threshold(current_step, total_steps, start_val=0.9, end_val=0.2):\n",
        "    \"\"\"\n",
        "    Linearly anneals the threshold from start_val down to end_val.\n",
        "    \"\"\"\n",
        "    if current_step >= total_steps:\n",
        "        return end_val\n",
        "\n",
        "    progress = current_step / total_steps\n",
        "    current_val = start_val - (progress * (start_val - end_val))\n",
        "    return current_val"
      ],
      "metadata": {
        "id": "KoGlsKHTPqDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator Rejection Sampling"
      ],
      "metadata": {
        "id": "PyMNIpVgPz_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Blindly trusting the topological sort is risky. Some sorts might preserve *dependencies* but break *discourse coherence* (e.g., \"However,\" appearing at the start).\n",
        "\n",
        "* **Add a small Perplexity Filter:**\n",
        "\n",
        "    1. Generate 10 topological sorts.\n",
        "\n",
        "    2. Run a tiny, fixed model (e.g., `distilgpt2`) or the current model itself (with `no_grad`) on them.\n",
        "\n",
        "    3. Calculate the perplexity of the generated sentences.\n",
        "\n",
        "    4. **Discard** any permutation where `Perplexity > 2.0 * Original_Perplexity`.\n",
        "\n",
        "    5. Train only on the \"plausible\" synthetic data.\n"
      ],
      "metadata": {
        "id": "oRPoVi7gQHF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "judge_name = \"distilgpt2\"\n",
        "judge_tokenizer = AutoTokenizer.from_pretrained(judge_name)\n",
        "judge_model = AutoModelForCausalLM.from_pretrained(judge_name).to(DEVICE)\n",
        "judge_model.eval()\n",
        "\n",
        "def calculate_perplexity(text, model, tokenizer):\n",
        "    \"\"\"Calculates perplexity (PPL) of a string.\"\"\"\n",
        "    encodings = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings, labels=encodings[\"input_ids\"])\n",
        "    return torch.exp(outputs.loss).item()"
      ],
      "metadata": {
        "id": "MNdr4D5PQC1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contrastive Consistency Loss"
      ],
      "metadata": {
        "id": "a441hWK4QvPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of just treating the synthetic data as \"more text to predict,\" treat it as a **semantic anchor**.\n",
        "\n",
        "* **Idea:** The embedding of the *last token* of the document should be similar for the Original and the Permutation (since they theoretically mean the same thing).\n",
        "\n",
        "* **Implementation:** Add a loss term: $L_{total} = L_{CLM} + \\lambda \\cdot MSE(H_{original}, H_{permuted})$.\n",
        "\n",
        "* **Why:** This forces the model to learn representations that are robust to sentence reordering, effectively teaching it to ignore \"positional noise.\""
      ],
      "metadata": {
        "id": "0awf9d5QRJjM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ITSnZmlTQh7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soft-Edge Sampling (Probabilistic DAG)"
      ],
      "metadata": {
        "id": "wGDCtVxfQyxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently, we binary threshold the edges (Edge exists OR it doesn't).\n",
        "\n",
        "* **Idea:** Treat the attention score as a probability $P(edge)$.\n",
        "\n",
        "* **Algorithm:** When building the DAG, add edge $A \\to B$ with probability proportional to `Attention(B, A)`.\n",
        "\n",
        "* **Effect:** This naturally handles \"maybe\" dependencies. Sometimes the graph will be strict, sometimes loose. This adds noise to the structure generation itself, which acts as a stronger regularizer.\n"
      ],
      "metadata": {
        "id": "ruCh1zdoRgph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_permutations_probabilistic(sent_data, attn_matrix, n_samples=3):\n",
        "    \"\"\"\n",
        "    Builds a DAG where edges are added probabilistically based on attention strength.\n",
        "    \"\"\"\n",
        "    G = nx.DiGraph()\n",
        "    n = len(sent_data)\n",
        "    for i in range(n):\n",
        "        G.add_node(i, text=sent_data[i]['text'])\n",
        "\n",
        "    # 1. Get stats to normalize probabilities\n",
        "    lower_triangle = attn_matrix[np.tril_indices(n, k=-1)]\n",
        "\n",
        "    if len(lower_triangle) > 0:\n",
        "        min_attn = np.min(lower_triangle)\n",
        "        max_attn = np.max(lower_triangle)\n",
        "        range_attn = max_attn - min_attn + 1e-9 # Avoid div/0\n",
        "    else:\n",
        "        # Fallback for single sentence or empty matrix\n",
        "        return G, [list(range(n))]\n",
        "\n",
        "    # 2. Add Edges Probabilistically\n",
        "    for i in range(n):       # Later sentence\n",
        "        for j in range(i):   # Earlier sentence\n",
        "            raw_score = attn_matrix[i, j]\n",
        "\n",
        "            # Normalize score to 0.0 - 1.0 range (Min-Max scaling)\n",
        "            # Stronger attention = Higher probability of edge existence\n",
        "            prob = (raw_score - min_attn) / range_attn\n",
        "\n",
        "            # Sampling: Roll the dice\n",
        "            if random.random() < prob:\n",
        "                G.add_edge(j, i)\n",
        "\n",
        "    # 3. Sample from this specific graph instance\n",
        "    # Note: With soft edges, you might get a different graph every time.\n",
        "    # Ideally, you regenerate the Graph K times, or sample K sorts from this one Graph.\n",
        "    # Here we sample K sorts from this one instance.\n",
        "\n",
        "    unique_permutations = set()\n",
        "    unique_permutations.add(tuple(range(n))) # Always keep original\n",
        "\n",
        "    attempts = 0\n",
        "    while len(unique_permutations) < (n_samples + 1) and attempts < (n_samples * 10):\n",
        "        new_perm = sample_topological_sort(G) # Use the sampler function from previous step\n",
        "        unique_permutations.add(tuple(new_perm))\n",
        "        attempts += 1\n",
        "\n",
        "    return G, [list(p) for p in unique_permutations]"
      ],
      "metadata": {
        "id": "s1U24mFcRTwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Training Loop"
      ],
      "metadata": {
        "id": "TG-RmF_aQida"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(NUM_STEPS):\n",
        "    try:\n",
        "        # 1. Fetch Data\n",
        "        sample = next(data_iter)\n",
        "        text = sample['text']\n",
        "\n",
        "        # Skip garbage/short data\n",
        "        if len(text) < 200:\n",
        "            continue\n",
        "\n",
        "        # 2. Preprocessing & Alignment\n",
        "        # We need the inputs on the device for the 'diagnostic' pass\n",
        "        inputs, sent_data = get_sentence_token_spans(text, tokenizer)\n",
        "        if len(sent_data) < 5: # Need enough sentences to permute\n",
        "            continue\n",
        "\n",
        "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "\n",
        "        # 3. Diagnostic Pass (No Grad)\n",
        "        # We put model in eval temporarily to get clean attention maps\n",
        "        model.eval()\n",
        "        attn_matrix = extract_sentence_dependency_matrix(model, inputs, sent_data)\n",
        "        model.train() # Switch back to train\n",
        "\n",
        "        # generate synthetic data\n",
        "        current_threshold = get_dynamic_threshold(step, NUM_STEPS)\n",
        "        _, permutations = generate_permutations(\n",
        "            sent_data,\n",
        "            attn_matrix,\n",
        "            quantile_threshold=current_threshold,\n",
        "            n_samples=PERMUTATIONS_PER_DOC\n",
        "        )\n",
        "\n",
        "        # 5. Prepare Training Batch\n",
        "        # We re-tokenize the permutations.\n",
        "        # Why? Because BPE merges might change when sentence neighbors change.\n",
        "        batch_texts = []\n",
        "        for perm_indices in permutations:\n",
        "            # Reconstruct string\n",
        "            perm_text = \" \".join([sent_data[idx]['text'] for idx in perm_indices])\n",
        "            batch_texts.append(perm_text)\n",
        "\n",
        "        # Tokenize the batch of (1 Real + K Synthetic)\n",
        "        batch_inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=1024\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # 6. Training Step\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass on the mixed batch\n",
        "        outputs = model(**batch_inputs, labels=batch_inputs[\"input_ids\"])\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping to prevent exploding gradients from bad permutations\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # 7. Logging\n",
        "        current_loss = loss.item()\n",
        "        running_loss += current_loss\n",
        "        batch_tokens = batch_inputs[\"input_ids\"].numel()\n",
        "        total_tokens_processed += batch_tokens\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            if len(permutations) > 1:\n",
        "              original_indices = permutations[0]\n",
        "              synthetic_indices = permutations[1]\n",
        "\n",
        "              # Calculate Kendall Tau\n",
        "              # 1.0 = Same order, -1.0 = Reverse order, 0.0 = Random\n",
        "              tau, _ = kendalltau(original_indices, synthetic_indices)\n",
        "\n",
        "              # Convert to a \"Scramble Score\" (0 = Identical, 1 = Very scrambled)\n",
        "              # Using (1 - tau) / 2 to normalize\n",
        "              scramble_score = (1 - tau) / 2\n",
        "\n",
        "              print(f\"   > Debug: Scramble Score: {scramble_score:.2f} (Tau: {tau:.2f})\")\n",
        "\n",
        "            avg_loss = running_loss / 10 if step > 0 else current_loss\n",
        "            elapsed = time.time() - start_time\n",
        "            tokens_per_sec = total_tokens_processed / elapsed\n",
        "\n",
        "            print(f\"Step {step} | Loss: {avg_loss:.4f} | Tok/s: {tokens_per_sec:.0f} | \"\n",
        "                  f\"Permutations: {len(batch_texts)}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    except StopIteration:\n",
        "        print(\"Dataset exhausted.\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping step due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "Fj43mGqZQllj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}