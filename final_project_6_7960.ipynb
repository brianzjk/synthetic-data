{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgF3RufRwN1HKIhzOlrmx+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Sy9dOSeXGx8"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "from datasets import load_dataset\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import spacy\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "Loading in model (we use GPT-2)"
      ],
      "metadata": {
        "id": "f54n6fOFXbHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "config = AutoConfig.from_pretrained(model_name, output_attentions=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, config=config)\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "KJL7vcC3XQUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_token_spans(text, tokenizer):\n",
        "    \"\"\"\n",
        "    Splits text into sentences and maps them to token ranges.\n",
        "    Returns: List of (sentence_text, start_token_idx, end_token_idx)\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "    # Tokenize the full text with offsets to align\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True)\n",
        "    offset_mapping = inputs.offset_mapping[0].numpy() # (num_tokens, 2)\n",
        "\n",
        "    sent_spans = []\n",
        "    current_sent_idx = 0\n",
        "\n",
        "    # Simple alignment: Find which tokens fall into the char range of the sentence\n",
        "    # Note: This is a greedy alignment; production code needs robust char-to-token mapping\n",
        "    char_cursor = 0\n",
        "    for sent in sentences:\n",
        "        start_char = text.find(sent, char_cursor)\n",
        "        end_char = start_char + len(sent)\n",
        "        char_cursor = end_char\n",
        "\n",
        "        # Find tokens that overlap with this sentence range\n",
        "        sent_token_indices = []\n",
        "        for i, (start, end) in enumerate(offset_mapping):\n",
        "            # Check overlap\n",
        "            if start >= start_char and end <= end_char:\n",
        "                sent_token_indices.append(i)\n",
        "\n",
        "        if sent_token_indices:\n",
        "            sent_spans.append({\n",
        "                \"text\": sent,\n",
        "                \"start\": sent_token_indices[0],\n",
        "                \"end\": sent_token_indices[-1]\n",
        "            })\n",
        "\n",
        "    return inputs, sent_spans"
      ],
      "metadata": {
        "id": "5Z-uehJoXfBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tester for the above code"
      ],
      "metadata": {
        "id": "ygh6FgA7mEmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"The CPU handles the instructions. \"\n",
        "    \"The RAM stores the data temporarily. \"\n",
        "    \"However, the GPU is needed for rendering images. \"\n",
        "    \"Together, these components power the computer. \"\n",
        "    \"The user interacts via the keyboard.\"\n",
        ")\n",
        "\n",
        "inputs, sent_data = get_sentence_token_spans(text, tokenizer)\n",
        "print(f\"Detected {len(sent_data)} sentences.\")"
      ],
      "metadata": {
        "id": "Auc8_PZHl1T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code runs the model once, extracts the attention, and then changes the attention from a (tokens x tokens) matrix to the sentence matrix (sentences x sentences)."
      ],
      "metadata": {
        "id": "5CvGT_0FmL13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sentence_dependency_matrix(model, inputs, sent_data, agg_method='mean'):\n",
        "    \"\"\"\n",
        "    Runs the model and aggregates token-level attention into sentence-level attention.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "    # outputs.attentions is a tuple of (num_layers) tensors of shape (batch, heads, seq, seq)\n",
        "    # We will average across all layers and heads for a 'global' attention view.\n",
        "    # You could also target specific 'induction heads' if you wanted to be fancy.\n",
        "\n",
        "    # Stack layers -> (layers, batch, heads, seq, seq)\n",
        "    all_attentions = torch.stack(outputs.attentions)\n",
        "\n",
        "    # Average over layers and heads -> (seq, seq)\n",
        "    # Squeeze batch dimension (0)\n",
        "    avg_attention = all_attentions.mean(dim=(0, 2)).squeeze(0).cpu().numpy()\n",
        "\n",
        "    n_sentences = len(sent_data)\n",
        "    sent_attn_matrix = np.zeros((n_sentences, n_sentences))\n",
        "\n",
        "    for i in range(n_sentences): # Query Sentence (The one 'looking back')\n",
        "        for j in range(n_sentences): # Key Sentence (The one being looked at)\n",
        "\n",
        "            # Get token ranges\n",
        "            q_start, q_end = sent_data[i]['start'], sent_data[i]['end']\n",
        "            k_start, k_end = sent_data[j]['start'], sent_data[j]['end']\n",
        "\n",
        "            # Extract sub-matrix for these two sentences\n",
        "            # Note: causal masking means attn is 0 if k > q, handled naturally by the model\n",
        "            sub_matrix = avg_attention[q_start:q_end+1, k_start:k_end+1]\n",
        "\n",
        "            if sub_matrix.size > 0:\n",
        "                if agg_method == 'max':\n",
        "                    score = np.max(sub_matrix)\n",
        "                elif agg_method == 'mean':\n",
        "                    score = np.mean(sub_matrix)\n",
        "                elif agg_method == 'sum':\n",
        "                    score = np.sum(sub_matrix)\n",
        "\n",
        "                sent_attn_matrix[i, j] = score\n",
        "\n",
        "    return sent_attn_matrix"
      ],
      "metadata": {
        "id": "XItnuJCHma6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_matrix = extract_sentence_dependency_matrix(model, inputs, sent_data, agg_method='mean')"
      ],
      "metadata": {
        "id": "BTvNGZ_3mdpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of Sentence Attention\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(attn_matrix, cmap='viridis')\n",
        "plt.title(\"Sentence-to-Sentence Attention (Raw)\")\n",
        "plt.xlabel(\"Key Sentence (Source Info)\")\n",
        "plt.ylabel(\"Query Sentence (Destination Info)\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DjxACTr7mfEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we construct the DAG of the attentions. In this DAG, there's an edge between two sentences if the attention is above some certain threshold. This is because any two sentences will likely have some small attention, so we only want to filter for sentence pairs that are highly important to each other."
      ],
      "metadata": {
        "id": "G-GMXHoqnKyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_permutations(sent_data, attn_matrix, quantile_threshold=0.2, n_samples=5):\n",
        "    \"\"\"\n",
        "    Builds the dependency DAG and samples 'n_samples' unique permutations.\n",
        "    \"\"\"\n",
        "    G = nx.DiGraph()\n",
        "    n = len(sent_data)\n",
        "\n",
        "    # 1. Add Nodes\n",
        "    for i in range(n):\n",
        "        G.add_node(i, text=sent_data[i]['text'])\n",
        "\n",
        "    # 2. Determine Threshold\n",
        "    lower_triangle = attn_matrix[np.tril_indices(n, k=-1)]\n",
        "    if len(lower_triangle) > 0:\n",
        "        cutoff = np.quantile(lower_triangle, quantile_threshold)\n",
        "    else:\n",
        "        cutoff = 0.0\n",
        "\n",
        "    # 3. Add Edges (Pruning weak attention)\n",
        "    # Edge j -> i means i depends on j\n",
        "    for i in range(n): # Later sentence\n",
        "        for j in range(i): # Earlier sentence\n",
        "            if attn_matrix[i, j] > cutoff:\n",
        "                G.add_edge(j, i)\n",
        "\n",
        "    # 4. Sample Permutations\n",
        "    unique_permutations = set()\n",
        "    attempts = 0\n",
        "    max_attempts = n_samples * 5 # Stop if we can't find enough unique ones\n",
        "\n",
        "    # Add the original sequence as the first baseline \"permutation\"\n",
        "    original_seq = tuple(range(n))\n",
        "    unique_permutations.add(original_seq)\n",
        "\n",
        "    while len(unique_permutations) < (n_samples + 1) and attempts < max_attempts:\n",
        "        # Call the internal sampler\n",
        "        new_perm = sample_topological_sort(G)\n",
        "        unique_permutations.add(tuple(new_perm))\n",
        "        attempts += 1\n",
        "\n",
        "    # Convert sets of tuples back to lists\n",
        "    results = [list(p) for p in unique_permutations]\n",
        "\n",
        "    return G, results"
      ],
      "metadata": {
        "id": "CuIVqbAemiiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there's $O(n!)$ possible permutations, we limit the permutations we use to some random sample of all valid topological ordering of the vertices of the DAG."
      ],
      "metadata": {
        "id": "KOF1isB3nfVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_topological_sort(G):\n",
        "    \"\"\"\n",
        "    Generates a single random topological sort of a DAG using\n",
        "    Randomized Kahn's Algorithm.\n",
        "\n",
        "    Args:\n",
        "        G: A NetworkX DiGraph.\n",
        "    Returns:\n",
        "        A list of node indices representing a valid permutation.\n",
        "    \"\"\"\n",
        "    in_degree = {u: d for u, d in G.in_degree() if d > 0}\n",
        "\n",
        "    # Initialize queue with nodes having 0 in-degree (no dependencies)\n",
        "    zero_in_degree_queue = [n for n in G.nodes() if G.in_degree(n) == 0]\n",
        "\n",
        "    sorted_order = []\n",
        "\n",
        "    while zero_in_degree_queue:\n",
        "        # Pick a random candidate from the available \"next steps\"\n",
        "        candidate_idx = random.randrange(len(zero_in_degree_queue))\n",
        "        current_node = zero_in_degree_queue.pop(candidate_idx)\n",
        "\n",
        "        sorted_order.append(current_node)\n",
        "\n",
        "        # 'Virtual' removal of the node: update neighbor in-degrees\n",
        "        for neighbor in G.successors(current_node):\n",
        "            in_degree[neighbor] -= 1\n",
        "            if in_degree[neighbor] == 0:\n",
        "                zero_in_degree_queue.append(neighbor)\n",
        "                del in_degree[neighbor]\n",
        "\n",
        "    # Cycle detection fallback\n",
        "    if len(sorted_order) != len(G.nodes()):\n",
        "        # This usually means the graph had a cycle.\n",
        "        # Fallback: return the nodes sorted by their original index (0, 1, 2...)\n",
        "        return sorted(list(G.nodes()))\n",
        "\n",
        "    return sorted_order"
      ],
      "metadata": {
        "id": "CMQ-1i02pfgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dag, permutations = generate_permutations(\n",
        "    sent_data,\n",
        "    attn_matrix,\n",
        "    quantile_threshold=0.5,\n",
        "    n_samples=3\n",
        ")\n",
        "\n",
        "print(f\"Generated {len(permutations)} unique sequences (including original).\\n\")"
      ],
      "metadata": {
        "id": "riSlbeEUnR0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing out 3 examples of the generated sentences:"
      ],
      "metadata": {
        "id": "HaAi-TvmqCij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, perm in enumerate(permutations[:3]):\n",
        "    print(f\"--- Permutation {i+1} ---\")\n",
        "    reconstructed_text = \" \".join([sent_data[idx]['text'] for idx in perm])\n",
        "    print(reconstructed_text)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "lQZmWL1bnYV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a visualization of what the DAG looks like for this example."
      ],
      "metadata": {
        "id": "LMVokRXwoF5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "pos = nx.spring_layout(dag)\n",
        "nx.draw(dag, pos, with_labels=True, node_color='lightblue', edge_color='gray', arrowsize=20)\n",
        "plt.title(\"Semantic Dependency DAG\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vdW6hk6cnbcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real Data Example\n",
        "Now that we have a working setup, we can try this on some synthetic data!"
      ],
      "metadata": {
        "id": "O1CUuU5NqV3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_loader(dataset_name=\"wikitext\", subset=\"wikitext-103-raw-v1\", split=\"train\"):\n",
        "    \"\"\"\n",
        "    Returns an iterable streaming dataset.\n",
        "    \"\"\"\n",
        "    # streaming=True means we don't download the 100GB+ file to disk\n",
        "    dataset = load_dataset(dataset_name, subset, split=split, streaming=True)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "_u7jHC_vqgPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize loader\n",
        "raw_dataset = get_data_loader()\n",
        "\n",
        "# Create a generator to fetch documents one by one\n",
        "data_iter = iter(raw_dataset)"
      ],
      "metadata": {
        "id": "L0N5-pPUqlac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: you need to run the below cell multiple times until you get something that actually looks like part of a Wikipedia article."
      ],
      "metadata": {
        "id": "t7EOgi2WsQRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fetching document from Wikitext-103...\")\n",
        "sample = next(data_iter)\n",
        "doc_text = sample['text']\n",
        "print(f\"\\nOriginal Doc Start: {doc_text[:100]}...\\n\")"
      ],
      "metadata": {
        "id": "vi1vDagxqk4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating the permutations!"
      ],
      "metadata": {
        "id": "gkw_0xt7sVXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, sent_data = get_sentence_token_spans(doc_text, tokenizer)\n",
        "print(f\"Detected {len(sent_data)} sentences.\")\n",
        "\n",
        "attn_matrix = extract_sentence_dependency_matrix(model, inputs, sent_data)\n",
        "dag, permutations = generate_permutations(sent_data, attn_matrix, quantile_threshold=0.5)\n",
        "\n",
        "print(f\"Generated {len(permutations)} permutations (including original).\")"
      ],
      "metadata": {
        "id": "-pLunqi_ndOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, perm in enumerate(permutations[:3]):\n",
        "    print(f\"--- Permutation {i+1} ---\")\n",
        "    reconstructed_text = \" \".join([sent_data[idx]['text'] for idx in perm])\n",
        "    print(reconstructed_text)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "Qs0vGtEisX-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "pos = nx.spring_layout(dag)\n",
        "nx.draw(dag, pos, with_labels=True, node_color='lightblue', edge_color='gray', arrowsize=20)\n",
        "plt.title(\"Semantic Dependency DAG\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iBnp_9zTq-2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "D5CWNrw7tsri"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nvrlZ9purApf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}